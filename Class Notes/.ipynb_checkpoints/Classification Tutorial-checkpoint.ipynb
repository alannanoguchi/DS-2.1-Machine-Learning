{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: The Logistic Classification Function\n",
    "A logistic model extends the logistic regression model into a classifier, meaning that now the logistic function can be used to classify continuous values into discretized categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# this dataset is loaded from the scikit-learn website\n",
    "iris_data = load_iris()\n",
    "\n",
    "# show descriptive information on the dataset\n",
    "print(iris_data.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                5.1               3.5                1.4               0.2\n",
       "1                4.9               3.0                1.4               0.2\n",
       "2                4.7               3.2                1.3               0.2\n",
       "3                4.6               3.1                1.5               0.2\n",
       "4                5.0               3.6                1.4               0.2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "X, y = pd.DataFrame(data=iris_data.data, columns=iris_data.feature_names), pd.DataFrame(data=iris_data.target, columns=[\"iris_type\"])\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Each feature in our dataset contains continuous data, which is essential for appropriate regression calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iris_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   iris_type\n",
       "0          0\n",
       "1          0\n",
       "2          0\n",
       "3          0\n",
       "4          0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We'll be using a module called train_test_split() that allows us to randomly partition our data. This will make more sense as we put it in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "# This final command is needed to restructure our y-data in order to effectively fit and predict the model on it.\n",
    "y_train, y_test = np.ravel(y_train), np.ravel(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By comparing the predicted y-values with our true test y-values, we can ascertain our model's accuracy.\n",
    "\n",
    "One way to do this is by manually iterating across our predicted y-values (y_pred) and true test y-values (y_test) and checking which values are equivalent.\n",
    "\n",
    "However, we can do this by simply calling .score() on our machine learning model.\n",
    "\n",
    "Keep in mind, two important arguments we send as parameters to our model are the solver and the multi_class parameters.\n",
    "\n",
    "The parameter passed for the solver case is called the Limited-Memory BFGS, which is a popular optimization algorithm useful in machine learning.\n",
    "\n",
    "The parameter passed for the multi_class case is called multinomial, which tells the model that the logistic regression expects discretized cases that are over two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
       "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression(random_state=0, solver=\"lbfgs\", multi_class=\"multinomial\")\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here, we've just instantiated our machine learning model (our logistic regression model) by assigning an empty model to a variable.\n",
    "\n",
    "Then, we fitted the model to our training dataset. Here, it learns the approximate relationship between the X and y datasets.\n",
    "\n",
    "Through learning the relationship between the X-y training data, we're hoping that the model can approximately determine the y-value given a new X-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 0, 0, 2, 1,\n",
       "       0, 0, 2, 0, 0, 1, 1, 0, 2, 1, 0, 2, 2, 1, 0, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = logreg.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By printing out and seeing the values across y_pred, we see what our model thinks should be the correct target labels for the corresponding X_test values.\n",
    "\n",
    "A quick sanity check we can do to assure that our data is what we think it is is to call .shape on y_pred and y_test.\n",
    "\n",
    "If the shape of both the predicted y-values array and the true test y-values array are consistent with one another, then we can assume that our model worked somewhat effectively.\n",
    "\n",
    "We can also use the .predict_proba() command to grab the relative classification probabilities in an array, rather than the assigned classes themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.17924703e-04, 5.61479667e-02, 9.43734109e-01],\n",
       "       [1.26288661e-02, 9.60454922e-01, 2.69162124e-02],\n",
       "       [9.84397680e-01, 1.56022816e-02, 3.85650267e-08],\n",
       "       [1.25180832e-06, 2.31530394e-02, 9.76845709e-01],\n",
       "       [9.70234755e-01, 2.97650820e-02, 1.62609745e-07],\n",
       "       [2.01669798e-06, 5.94453033e-03, 9.94053453e-01],\n",
       "       [9.81899481e-01, 1.81004487e-02, 7.04478339e-08],\n",
       "       [2.84241427e-03, 7.47089885e-01, 2.50067701e-01],\n",
       "       [1.50915665e-03, 7.38524267e-01, 2.59966577e-01],\n",
       "       [2.05287874e-02, 9.35891198e-01, 4.35800150e-02],\n",
       "       [9.22436289e-05, 1.59475749e-01, 8.40432007e-01],\n",
       "       [6.98627957e-03, 8.09989247e-01, 1.83024474e-01],\n",
       "       [4.08220400e-03, 7.93602802e-01, 2.02314994e-01],\n",
       "       [3.05681845e-03, 7.60910824e-01, 2.36032358e-01],\n",
       "       [3.87699846e-03, 7.10277106e-01, 2.85845895e-01],\n",
       "       [9.82815573e-01, 1.71843701e-02, 5.65491187e-08],\n",
       "       [6.72901329e-03, 7.56465383e-01, 2.36805604e-01],\n",
       "       [1.14291723e-02, 8.45111094e-01, 1.43459734e-01],\n",
       "       [9.67582059e-01, 3.24177265e-02, 2.14247098e-07],\n",
       "       [9.82872119e-01, 1.71278214e-02, 5.96916585e-08],\n",
       "       [8.34498625e-04, 1.93259944e-01, 8.05905557e-01],\n",
       "       [1.03255850e-02, 7.11146975e-01, 2.78527440e-01],\n",
       "       [9.44128789e-01, 5.58701622e-02, 1.04843253e-06],\n",
       "       [9.75498454e-01, 2.45013784e-02, 1.67528701e-07],\n",
       "       [1.36907647e-03, 4.26372101e-01, 5.72258822e-01],\n",
       "       [9.94203363e-01, 5.79662703e-03, 9.65350921e-09],\n",
       "       [9.50240553e-01, 4.97583147e-02, 1.13246715e-06],\n",
       "       [1.07122576e-02, 9.00994901e-01, 8.82928415e-02],\n",
       "       [1.40884689e-01, 8.52874363e-01, 6.24094861e-03],\n",
       "       [9.61491946e-01, 3.85076049e-02, 4.49537354e-07],\n",
       "       [9.90734933e-05, 1.15644583e-01, 8.84256343e-01],\n",
       "       [1.19870174e-02, 6.84359055e-01, 3.03653928e-01],\n",
       "       [9.68058424e-01, 3.19414261e-02, 1.50155312e-07],\n",
       "       [1.28526462e-03, 3.57780449e-01, 6.40934287e-01],\n",
       "       [1.48836451e-05, 3.38274124e-02, 9.66157704e-01],\n",
       "       [4.81304832e-02, 8.80739359e-01, 7.11301583e-02],\n",
       "       [9.44629255e-01, 5.53703539e-02, 3.91145203e-07],\n",
       "       [6.02626586e-04, 3.11033047e-01, 6.88364327e-01]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The next step is to simply determine the model's accuracy.\n",
    "\n",
    "We can do this by calling .score() on our model and sending it our test data.\n",
    "\n",
    "One common mistake many people make is to send y_pred rather than X_test as the first argument to the scoring method.\n",
    "\n",
    "In this case, since we have three explicit class labels, we technically have a 33.33% baseline probability to correctly assign any label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97.36842105263158"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_test, y_test) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
