{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "- So far, learned about two classifiers:\n",
    "\n",
    "    - SVM\n",
    "    \n",
    "    - Logistic Regression\n",
    "    \n",
    "- How we can evalute which model performs better for our classification task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity:  Write a function that returns \n",
    "\n",
    "TN, FP, FN, TP, Accuracy, Precision, Recall, F1-score\n",
    "\n",
    "* TN = True Negative(Actual:0, Predicted:0)\n",
    "\n",
    "* FP = False Positive(Actual:0, Predicted:1)\n",
    "\n",
    "* FN False Negative(Actual:1, Predicted:0)\n",
    "\n",
    "* TP = True Positive(Actual:1, Predicted:1)\n",
    "\n",
    "From Confusion Matrix\n",
    "\n",
    "- Hint:\n",
    "\n",
    "* Accuracy = ((TP + TN) / float(TP + TN + FP + FN))\n",
    "overall, how often is the classifier correct?\n",
    "\n",
    "* Precision = TP / float(TP + FP)\n",
    "when a positive value is predicted, how often is the prediction correct?\n",
    "\n",
    "* Recall = TP / float(FN + TP)\n",
    "when the actual value is positive, how often is the prediction correct?\n",
    "\n",
    "* Specificity = TN / (TN + FP)\n",
    "when the actual value is negative, how often is the prediction correct?\n",
    "\n",
    "* Classification Error = 1 - accuracy = FP + FN / (TP + TN+ FP + FN)\n",
    "overall, how often is this classifier incorrect\n",
    "\n",
    "\n",
    "* F1-score = 2 x (Precision x Recall )/(Precision + Recall)\n",
    "harmonic mean of precision and recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6927083333333334 0.5555555555555556 0.24193548387096775 0.3370786516853933\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "# confusion = metrics.confusion_matrix(y_test, y_pred)\n",
    "# print(confusion)\n",
    "\n",
    "confusion = np.array([[118., 12.], [ 47., 15.]])\n",
    "TN = confusion[0,0]     # confusion[actual, predicted]\n",
    "FP = confusion[0,1]\n",
    "TP = confusion[1,1]\n",
    "FN = confusion[1,0]\n",
    "\n",
    "accuracy = ((TP + TN) / float(TP + TN + FP + FN))\n",
    "precision = TP / float(TP + FP)\n",
    "recall = TP / float(FN + TP)\n",
    "# specificity = TN / (TN + FP)\n",
    "F1_score = 2*precision*recall/float(precision+recall)\n",
    "\n",
    "print(accuracy, precision, recall, F1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: How can we obtain y_pred (whose elements are 0 and 1) from:\n",
    "- `y-pred_prob` = logreg.predict_proba(X_test)\n",
    "- Goal: `y_pred_proba` --> `y_pred`\n",
    "- Steps: For given `y_pred_proba`, check if the first element is > 0.5 then returns 0 else returns 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_pred_elements(y_pred_prob):\n",
    "    y_pred = []\n",
    "    for i in y_pred_prob:\n",
    "        if i[1] < 0.3576388888888889:\n",
    "            y_pred.append(0)\n",
    "        else:\n",
    "            y_pred.append(1)\n",
    "            \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_y_pred = y_pred_elements(y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3576388888888889"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()[1]/len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[87, 43],\n",
       "       [24, 38]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test, new_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Change the threshold of Logistic Regression to get new (better) confusion matrix\n",
    "\n",
    "- The question now is which threshold is better?\n",
    "\n",
    "- To do this:\n",
    "\n",
    "1- Train `logreg = LogisticRegression()` and get `y_pred_prob = logreg.predict_proba(X_test)`\n",
    "\n",
    "2- The second column of `y_pred_prob` is the probability that a subject be diabetes \n",
    "\n",
    "3- Plot histogram of second column. Hint: `plt.hist(y_pred_prob[:, 1], bins=8) plt.show()`\n",
    "\n",
    "4- Count how many of `y_train` is 0 how many is 1. Define thershold as `threshold = y_train.value_counts()[1] / len(y_train)`\n",
    "\n",
    "5- Write a function that returns 0 if `y_pred_prob[:, 1]` is less than threshold, else returns 1\n",
    "\n",
    "6- Calculate the Confuction Matrix by `confusion = metrics.confusion_matrix(y_test, y_pred)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n",
      "       'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome'],\n",
      "      dtype='object')\n",
      "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
      "0            6      148             72             35        0  33.6   \n",
      "1            1       85             66             29        0  26.6   \n",
      "2            8      183             64              0        0  23.3   \n",
      "3            1       89             66             23       94  28.1   \n",
      "4            0      137             40             35      168  43.1   \n",
      "\n",
      "   DiabetesPedigreeFunction  Age  Outcome  \n",
      "0                     0.627   50        1  \n",
      "1                     0.351   31        0  \n",
      "2                     0.672   32        1  \n",
      "3                     0.167   21        0  \n",
      "4                     2.288   33        1  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pima = pd.read_csv('diabetes.csv')\n",
    "print(pima.columns)\n",
    "print(pima.head())\n",
    "\n",
    "feature_cols = ['Pregnancies', 'Insulin', 'BMI', 'Age']\n",
    "\n",
    "X = pima[feature_cols]\n",
    "# print(X)\n",
    "# y is a vector, hence we use dot to access 'label'\n",
    "y = pima['Outcome']\n",
    "\n",
    "# split X and y into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# fit model\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred_prob = logreg.predict_proba(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.63247571, 0.36752429],\n",
       "       [0.71643656, 0.28356344],\n",
       "       [0.71104114, 0.28895886],\n",
       "       [0.5858938 , 0.4141062 ],\n",
       "       [0.84103973, 0.15896027],\n",
       "       [0.82934844, 0.17065156],\n",
       "       [0.50110974, 0.49889026],\n",
       "       [0.48658459, 0.51341541],\n",
       "       [0.72321388, 0.27678612],\n",
       "       [0.32810562, 0.67189438],\n",
       "       [0.64244443, 0.35755557],\n",
       "       [0.25912035, 0.74087965],\n",
       "       [0.63949765, 0.36050235],\n",
       "       [0.76987637, 0.23012363],\n",
       "       [0.57345769, 0.42654231],\n",
       "       [0.80896485, 0.19103515],\n",
       "       [0.54236399, 0.45763601],\n",
       "       [0.8809859 , 0.1190141 ],\n",
       "       [0.56071047, 0.43928953],\n",
       "       [0.63038849, 0.36961151],\n",
       "       [0.55812011, 0.44187989],\n",
       "       [0.62388338, 0.37611662],\n",
       "       [0.80183978, 0.19816022],\n",
       "       [0.58322696, 0.41677304],\n",
       "       [0.84451719, 0.15548281],\n",
       "       [0.7468329 , 0.2531671 ],\n",
       "       [0.90256923, 0.09743077],\n",
       "       [0.30366288, 0.69633712],\n",
       "       [0.84641691, 0.15358309],\n",
       "       [0.7802164 , 0.2197836 ],\n",
       "       [0.56905168, 0.43094832],\n",
       "       [0.65783942, 0.34216058],\n",
       "       [0.77603886, 0.22396114],\n",
       "       [0.61926457, 0.38073543],\n",
       "       [0.86657866, 0.13342134],\n",
       "       [0.61209784, 0.38790216],\n",
       "       [0.52950297, 0.47049703],\n",
       "       [0.83795257, 0.16204743],\n",
       "       [0.70451824, 0.29548176],\n",
       "       [0.69081839, 0.30918161],\n",
       "       [0.72700295, 0.27299705],\n",
       "       [0.61183417, 0.38816583],\n",
       "       [0.72646557, 0.27353443],\n",
       "       [0.71118959, 0.28881041],\n",
       "       [0.36528086, 0.63471914],\n",
       "       [0.97634749, 0.02365251],\n",
       "       [0.84179352, 0.15820648],\n",
       "       [0.76981625, 0.23018375],\n",
       "       [0.6515407 , 0.3484593 ],\n",
       "       [0.72419959, 0.27580041],\n",
       "       [0.66735896, 0.33264104],\n",
       "       [0.75119404, 0.24880596],\n",
       "       [0.25510488, 0.74489512],\n",
       "       [0.60998536, 0.39001464],\n",
       "       [0.58374455, 0.41625545],\n",
       "       [0.86424313, 0.13575687],\n",
       "       [0.81104624, 0.18895376],\n",
       "       [0.35222318, 0.64777682],\n",
       "       [0.81077869, 0.18922131],\n",
       "       [0.94314096, 0.05685904],\n",
       "       [0.36008453, 0.63991547],\n",
       "       [0.53363618, 0.46636382],\n",
       "       [0.8749028 , 0.1250972 ],\n",
       "       [0.73042398, 0.26957602],\n",
       "       [0.75080896, 0.24919104],\n",
       "       [0.69429604, 0.30570396],\n",
       "       [0.53623776, 0.46376224],\n",
       "       [0.79036905, 0.20963095],\n",
       "       [0.57152171, 0.42847829],\n",
       "       [0.59237736, 0.40762264],\n",
       "       [0.79830904, 0.20169096],\n",
       "       [0.72972934, 0.27027066],\n",
       "       [0.73744144, 0.26255856],\n",
       "       [0.42761737, 0.57238263],\n",
       "       [0.54532959, 0.45467041],\n",
       "       [0.72283848, 0.27716152],\n",
       "       [0.41998719, 0.58001281],\n",
       "       [0.58400512, 0.41599488],\n",
       "       [0.72723899, 0.27276101],\n",
       "       [0.65900777, 0.34099223],\n",
       "       [0.45373422, 0.54626578],\n",
       "       [0.62069277, 0.37930723],\n",
       "       [0.7007795 , 0.2992205 ],\n",
       "       [0.89940831, 0.10059169],\n",
       "       [0.67127398, 0.32872602],\n",
       "       [0.54898637, 0.45101363],\n",
       "       [0.83963021, 0.16036979],\n",
       "       [0.5103025 , 0.4896975 ],\n",
       "       [0.36769492, 0.63230508],\n",
       "       [0.59261596, 0.40738404],\n",
       "       [0.80205603, 0.19794397],\n",
       "       [0.80301979, 0.19698021],\n",
       "       [0.75536792, 0.24463208],\n",
       "       [0.88852815, 0.11147185],\n",
       "       [0.5841403 , 0.4158597 ],\n",
       "       [0.78438144, 0.21561856],\n",
       "       [0.45875471, 0.54124529],\n",
       "       [0.51196398, 0.48803602],\n",
       "       [0.35347233, 0.64652767],\n",
       "       [0.66059342, 0.33940658],\n",
       "       [0.45736573, 0.54263427],\n",
       "       [0.83786176, 0.16213824],\n",
       "       [0.6221259 , 0.3778741 ],\n",
       "       [0.88688713, 0.11311287],\n",
       "       [0.65218013, 0.34781987],\n",
       "       [0.65957216, 0.34042784],\n",
       "       [0.8209015 , 0.1790985 ],\n",
       "       [0.78675188, 0.21324812],\n",
       "       [0.85289054, 0.14710946],\n",
       "       [0.76985898, 0.23014102],\n",
       "       [0.81595408, 0.18404592],\n",
       "       [0.47775351, 0.52224649],\n",
       "       [0.52900634, 0.47099366],\n",
       "       [0.71115752, 0.28884248],\n",
       "       [0.50674921, 0.49325079],\n",
       "       [0.58255527, 0.41744473],\n",
       "       [0.77084992, 0.22915008],\n",
       "       [0.72977089, 0.27022911],\n",
       "       [0.80756076, 0.19243924],\n",
       "       [0.2501287 , 0.7498713 ],\n",
       "       [0.53499907, 0.46500093],\n",
       "       [0.3354546 , 0.6645454 ],\n",
       "       [0.57901401, 0.42098599],\n",
       "       [0.46435966, 0.53564034],\n",
       "       [0.83965298, 0.16034702],\n",
       "       [0.8564314 , 0.1435686 ],\n",
       "       [0.61857574, 0.38142426],\n",
       "       [0.66172686, 0.33827314],\n",
       "       [0.6369935 , 0.3630065 ],\n",
       "       [0.87157469, 0.12842531],\n",
       "       [0.71666307, 0.28333693],\n",
       "       [0.95994442, 0.04005558],\n",
       "       [0.81518861, 0.18481139],\n",
       "       [0.33283053, 0.66716947],\n",
       "       [0.53647126, 0.46352874],\n",
       "       [0.51284318, 0.48715682],\n",
       "       [0.80089206, 0.19910794],\n",
       "       [0.54138349, 0.45861651],\n",
       "       [0.76783279, 0.23216721],\n",
       "       [0.81630733, 0.18369267],\n",
       "       [0.73608006, 0.26391994],\n",
       "       [0.62507031, 0.37492969],\n",
       "       [0.87083494, 0.12916506],\n",
       "       [0.58586087, 0.41413913],\n",
       "       [0.57539142, 0.42460858],\n",
       "       [0.86167809, 0.13832191],\n",
       "       [0.79218306, 0.20781694],\n",
       "       [0.70522301, 0.29477699],\n",
       "       [0.84174901, 0.15825099],\n",
       "       [0.63983766, 0.36016234],\n",
       "       [0.76258551, 0.23741449],\n",
       "       [0.56649311, 0.43350689],\n",
       "       [0.79380119, 0.20619881],\n",
       "       [0.76837662, 0.23162338],\n",
       "       [0.38888459, 0.61111541],\n",
       "       [0.80268991, 0.19731009],\n",
       "       [0.19928502, 0.80071498],\n",
       "       [0.82191509, 0.17808491],\n",
       "       [0.63511265, 0.36488735],\n",
       "       [0.21381357, 0.78618643],\n",
       "       [0.55919386, 0.44080614],\n",
       "       [0.63440346, 0.36559654],\n",
       "       [0.88239862, 0.11760138],\n",
       "       [0.77156675, 0.22843325],\n",
       "       [0.52134931, 0.47865069],\n",
       "       [0.78679475, 0.21320525],\n",
       "       [0.48501479, 0.51498521],\n",
       "       [0.83877506, 0.16122494],\n",
       "       [0.76259881, 0.23740119],\n",
       "       [0.70625609, 0.29374391],\n",
       "       [0.83329952, 0.16670048],\n",
       "       [0.51283474, 0.48716526],\n",
       "       [0.70030106, 0.29969894],\n",
       "       [0.55348957, 0.44651043],\n",
       "       [0.49830098, 0.50169902],\n",
       "       [0.70753494, 0.29246506],\n",
       "       [0.38263772, 0.61736228],\n",
       "       [0.58406005, 0.41593995],\n",
       "       [0.74179055, 0.25820945],\n",
       "       [0.8258032 , 0.1741968 ],\n",
       "       [0.66480459, 0.33519541],\n",
       "       [0.30393175, 0.69606825],\n",
       "       [0.67545632, 0.32454368],\n",
       "       [0.64269574, 0.35730426],\n",
       "       [0.7663053 , 0.2336947 ],\n",
       "       [0.76261476, 0.23738524],\n",
       "       [0.61590682, 0.38409318],\n",
       "       [0.75308588, 0.24691412],\n",
       "       [0.72045448, 0.27954552],\n",
       "       [0.81498826, 0.18501174],\n",
       "       [0.7377638 , 0.2622362 ],\n",
       "       [0.72143074, 0.27856926]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.65625, 0.4791666666666667, 0.7419354838709677, 0.5822784810126582)\n",
      "(0.65625, 0.4791666666666667, 0.7419354838709677, 0.5822784810126582)\n"
     ]
    }
   ],
   "source": [
    "# With threshold 0.5\n",
    "print((Accuray, Precision, Recall, F1_score))\n",
    "\n",
    "#(0.6927083333333334, 0.5555555555555556, 0.24193548387096775, 0.3370786516853933)\n",
    "\n",
    "# With Optimal Theshold\n",
    "print((Accuray, Precision, Recall, F1_score))\n",
    "\n",
    "#(0.65625, 0.4791666666666667, 0.7419354838709677, 0.5822784810126582)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Change the threshold of Logistic Regression (y_pred_proba) to get new confusion matrix and better model performance\n",
    "\n",
    "- The question hnow is which threshold is better?\n",
    "- To do this:\n",
    "1- Train logreg = LogisticRegression() and get y_pred_prob = logreg.predict_proba(X_test)\n",
    "2- The second column of y_pred_prob is the probability that a subject has diabetes\n",
    "3- Plot histogram of second column. Hint: plt.hist(y_pred_prob[:,1], bins=8) plt.show()\n",
    "4- Count how many of y_train is 0 and how is 1. Define threshold as:\n",
    "    - ratio of positive class (diabetes) versus negative (OK) class\n",
    "    - threshold = y_train.value_counts()[1] / len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(y_pred_prob[:, 1], bins = 8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.distplot(y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1.\n",
      " 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0.\n",
      " 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0.\n",
      " 1. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1.\n",
      " 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0.\n",
      " 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import binarize\n",
    "\n",
    "y_pred_prob_class_1 = logreg.predict_proba(X_test)[:,1]\n",
    "y_pred_class_threshold = binarize(y_pred_prob_class_1.reshape(1,-1), 0.3576388888888889)[0]\n",
    "print(y_pred_class_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 1 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
      " 0 0 0 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 0 0 1 1 1 1 0 0 1 1 1 1 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 1 0\n",
      " 1 1 1 1 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 1 0 0 0\n",
      " 0 1 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1\n",
      " 0 1 1 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_test.values.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.65625, 0.4791666666666667, 0.7419354838709677, 0.5822784810126582)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "confusion = np.array([[80, 50], [16, 46]])\n",
    "\n",
    "# confusion = np.array([[87, 43], [24, 38]])\n",
    "\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]\n",
    "TP = confusion[1, 1]\n",
    "\n",
    "Accuray = ((TP + TN) / float(TP + TN + FP + FN))\n",
    "\n",
    "Precision = TP / float(TP + FP)\n",
    "\n",
    "Recall = TP / float(FN + TP)\n",
    "\n",
    "F1_score = 2*Precision*Recall/float(Precision+Recall)\n",
    "\n",
    "print((Accuray, Precision, Recall, F1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "\n",
    "- Normally in a machine learning process, data is divided into training and test sets; the training set is then used to train the model and the test set is used to evaluate the performance of a model \n",
    "\n",
    "- It is possible that the accuracy obtained on one test is very different to accuracy obtained on another test set using the same algorithm\n",
    "\n",
    "- To see the model performance, we use K-Fold Cross-Validation for performance evaluation where K is any number\n",
    "\n",
    "- Suppose we want to perform 5-fold cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets obtain Accuracy and F1-Score for 5-fold cross validation based on diabetes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "268\n",
      "[0.64935065 0.65584416 0.64935065 0.69281046 0.65359477]\n",
      "0.6601901366607248\n",
      "[0.578125   0.55462185 0.54237288 0.624      0.576     ]\n",
      "0.5750239460190857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score \n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pima = pd.read_csv('diabetes.csv')\n",
    "\n",
    "feature_cols = ['Pregnancies', 'Insulin', 'BMI', 'Age']\n",
    "\n",
    "# X is a matrix,access the features we want in feature_cols\n",
    "X = pima[feature_cols]\n",
    "\n",
    "# y is a vector, hence we use dot to access 'label'\n",
    "y = pima['Outcome']\n",
    "\n",
    "print(y.value_counts()[0])\n",
    "print(y.value_counts()[1])\n",
    "\n",
    "logreg = LogisticRegression(class_weight={1: 500/268})     # 500/258 is the total number of 0's over 1's in y_value\n",
    "#logreg = LogisticRegression(class_weight={1: y.value_counts()[0]/y.value_counts()[1]})\n",
    "# logreg = LogisticRegression()\n",
    "\n",
    "# cv = cross-validation\n",
    "all_accuracies = cross_val_score(estimator=logreg, X=X, y=y, cv=5, scoring='accuracy')\n",
    "print(all_accuracies)      # [0.64935065 0.65584416 0.64935065 0.69281046 0.65359477]\n",
    "print(all_accuracies.mean())      # 0.6601901366607248\n",
    "\n",
    "all_f1 = cross_val_score(estimator=logreg, X=X, y=y, cv=5, scoring='f1')\n",
    "print(all_f1)       # [0.578125   0.55462185 0.54237288 0.624      0.576     ]\n",
    "print(all_f1.mean())      # 0.5750239460190857"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    500\n",
       "1    268\n",
       "Name: Outcome, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "plt.hist(y, bins=20)\n",
    "plt.show()\n",
    "\n",
    "y_pd_series = pd.Series(y)\n",
    "y_pd_series.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to choose Models based on Cross-Validation\n",
    "* we want to have low vairance for CV --> pick a model with the lower variance\n",
    "* if both have low variance --> pick the model with the higher mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search for Parameter Selection\n",
    "* Machine learning models have hyper-parameters. To know which values of hyper-paramaeters give the best result we need grid search\n",
    "\n",
    "* Question: what does grid search mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'grid_search' from 'sklearn' (/opt/anaconda3/lib/python3.7/site-packages/sklearn/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-adbe71bd623c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Grid Search for Parameter Selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msvc_param_selection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnfolds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'grid_search' from 'sklearn' (/opt/anaconda3/lib/python3.7/site-packages/sklearn/__init__.py)"
     ]
    }
   ],
   "source": [
    "## Grid Search for Parameter Selection\n",
    "\n",
    "from sklearn import svm, grid_search\n",
    "\n",
    "def svc_param_selection(X, y, nfolds):\n",
    "    Cs = [0.001, 0.01, 0.1, 1, 10]\n",
    "    gammas = [0.001, 0.01, 0.1, 1]\n",
    "    param_grid = {'C': Cs, 'gamma' : gammas}\n",
    "    grid_search = GridSearchCV(svm.SVC(kernel='rbf'), param_grid, cv=nfolds)\n",
    "    grid_search.fit(X, y)\n",
    "    return grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
