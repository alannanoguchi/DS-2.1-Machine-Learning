{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "* PCA is a well-known algorith for Dimensionality Reduction\n",
    "* PCA:\n",
    "    - reduces the number of feautres while keeping the feature information\n",
    "    - PCA is a mathematical technique to reduce redundancy in the data\n",
    "    - removes correlations among features\n",
    "    - emphasizes variation of strong features, making the data easier to visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review Matrix multiplication:\n",
    "    * Matrix  A = ([2, 0], [1,5]) and vector v = np.array([3,4])\n",
    "    * np.dot(A,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6, 23])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = ([2, 0], [1,5])\n",
    "v = np.array([3,4])\n",
    "\n",
    "np.dot(A,v)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 20])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(v,A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EigenValue and Eigenvector of Matrix\n",
    "\n",
    "A matrix's eigenvalues and eigenvectors are what we will use for the scalar value(a) and vector(v) respectively\n",
    "\n",
    "* Eigenvecotr(v) i a vector whose direction remains unchanged when a linear tranformation is applied to it. They represent the rotation matrix.\n",
    "* Eigenvalues(a) represents the scalar value that is used such that when multiplied by v, gives the same value as Av\n",
    "\n",
    "For a given matrix A, we want to obtain a vector v and a scalar a value such that:\n",
    "\n",
    "Av = av"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a python function to obtain vector(v) and scalar(a) for a given matrix A\n",
    "hints:\n",
    "1. Before we find the vector and scalar, we need the eigenvalye and eigenvector of A. Give the same matrix A, we used above, see how numpys linalg.eig method could help you solve this\n",
    "2. To check your answer, mulitply A by of its vectors, adn then multiply a by the same vector and see if you get the same outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5. 2.]\n",
      "[[ 0.          0.9486833 ]\n",
      " [ 1.         -0.31622777]]\n"
     ]
    }
   ],
   "source": [
    "eig_value, eig_vector = np.linalg.eig(A)\n",
    "\n",
    "print(eig_value)\n",
    "print(eig_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that Av = av"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  1.8973666 ],\n",
       "       [ 5.        , -0.63245553]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# muplitply A with its first eigen-vector\n",
    "np.dot(A, eig_vector[::1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (2,2) and (1,2) not aligned: 2 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-67cbeca580cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# mupltiply A with its second eigen-vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meig_vector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (2,2) and (1,2) not aligned: 2 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "# mupltiply A with its second eigen-vector\n",
    "np.dot(A, eig_vector[::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.8973666 , -0.63245553])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multiply the other eigen-value of A with its associaed eigen-vector\n",
    "eig_value[1]*eig_vector[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: Which country is different from the others?\n",
    "\n",
    "https://setosa.io/ev/principal-component-analysis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-144.99315218   -2.53299944]\n",
      " [ 477.39163882  -58.90186182]\n",
      " [ -91.869339    286.08178613]\n",
      " [-240.52914764 -224.64692488]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# use pandas to read csv in the excel spreadsheet\n",
    "df = pd.read_excel('pca_uk.xlsx')\n",
    "\n",
    "# build a matrix for the feature values, not including the text labels\n",
    "X = np.array([df[i].values for i in df.columns if i != 'Features'])\n",
    "\n",
    "# print(X)\n",
    "\n",
    "# Calculate the PCA\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Find the principle components of 17 features\n",
    "X_r = pca.fit_transform(X)\n",
    "\n",
    "print(X_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's visualize the principle component\n",
    "\n",
    "for feature, (plot_x, plot_y) in enumerate(zip(X_r[:, 0], X_r[:,1])):\n",
    "    plt.scatter(plot_x, plot_y)\n",
    "    plt.text(plot_x+0.3, plot_y+0.3, df.columns[:-1][feature])\n",
    "\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much of the information in the original dataset is preserved in the principle components?\n",
    "* Hint: use pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-144.99315218   -2.53299944]\n",
      " [ 477.39163882  -58.90186182]\n",
      " [ -91.869339    286.08178613]\n",
      " [-240.52914764 -224.64692488]]\n",
      "[105073.34576714  45261.62487597]\n",
      "[0.67444346 0.29052475]\n",
      "[0.67444346 0.96496821]\n"
     ]
    }
   ],
   "source": [
    "# PCA computation by sklearn\n",
    "\n",
    "pca = PCA(n_components = 2)\n",
    "X_r = pca.fit_transform(X)\n",
    "\n",
    "print(X_r)\n",
    "print(pca.explained_variance_)\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.explained_variance_ratio_.cumsum())   # we find that 96% of the information is preserved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to calculate the correlation of the principle components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation of PCA Components:\n",
      "(-2.7755575615628914e-17, 1.0)\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats\n",
    "\n",
    "print(\"Correlation of PCA Components:\")\n",
    "print(scipy.stats.pearsonr(X_r[:, 0], X_r[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity(Reminder): Reverse the elements of a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "list = [1, 2, 3]\n",
    "\n",
    "print(list[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 2, 1]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reverse_f(ls):\n",
    "    N = len(ls)\n",
    "    reverse_ls = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        reverse_ls.append(ls[N - (i + 1)])\n",
    "    return reverse_ls\n",
    "\n",
    "reverse_f([1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: PCA Steps\n",
    "\n",
    "Follow the steps here and write a function that computes the principle component for iris dataset \n",
    "\n",
    "https://sebastianraschka.com/Articles/2014_pca_step_by_step.html\n",
    "\n",
    "Steps: \n",
    "\n",
    "1- Subtract column mean from feature matrix\n",
    "\n",
    "2- Calculate the covariance of centered matrix\n",
    "\n",
    "3- Calculate the eigenvalue and eigenvector of covariance matrix\n",
    "\n",
    "4- Return the first K (two for example) column of matrix multiplication of centerned matrix with eigenvector matrix\n",
    "\n",
    "\n",
    "Compare the result of custom function with PCA in sklearn\n",
    "\n",
    "Calculate the correlation of the first two principle component \n",
    "\n",
    "How much of the dataset information is preserved in the first two components?\n",
    "\n",
    "Hint: use `pca.explained_variance_ratio_`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Use the following matrix: X = np.array([[1, 1, 1], [1, 2, 1], [1, 3, 2], [1, 4, 3]])\n",
    "\n",
    "2- Subtract the column mean from the feature matrix -> this new matrix will be our centered matrix\n",
    "\n",
    "3- Calculate the covariance of the centered matrix (check out numpy's resources to see if there's a function that can do this for you: np.cov())\n",
    "\n",
    "4- calculate the eigenvalue and eigenvector of the covariance matrix. Remember how we did this in the previous activity. \n",
    "\n",
    "5- Sort the eigenvalues so that they are in decreasing order, and then find the top N (for example, 2) eigenvectors\n",
    "\n",
    "6- Dot multiply the centered matrix with the top N eigenvectors of the covariance matrix\n",
    "\n",
    "7- Compare the result with the function with PCA in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1]\n",
      " [1 2 1]\n",
      " [1 3 2]\n",
      " [1 4 3]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ],\n",
       "       [-2.03405201,  0.03900378],\n",
       "       [-1.48274273, -0.05350605]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import scipy\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "X = np.array([[1, 1, 1], [1, 2, 1], [1, 3, 2], [1, 4, 3]])\n",
    "print(X)\n",
    "\n",
    "# PCA Computation by Function\n",
    "def PCA_calculation(data, n_comp=2):\n",
    "    \n",
    "    x_bar = np.mean(data, axis=0)\n",
    "    \n",
    "    # 2- Subtract the column mean from the feature matrix -> this new matrix will be our centered matrix\n",
    "    centered = data - x_bar\n",
    "    \n",
    "    # 3- Calculate the covariance of the centered matrix\n",
    "    covariance = np.cov(centered.T)     # .T = transpose -> the rows of the original data are now the columns\n",
    "    \n",
    "    # 4- calculate the eigenvalue and eigenvector of the covariance matrix\n",
    "    eig_value, eig_vector = np.linalg.eig(covariance)\n",
    "    \n",
    "    # 5- Sort the eigenvalues so that they are in decreasing order, and then find the top N (for example, 2) eigenvectors\n",
    "    idx = np.argsort(eig_value)[::-1]\n",
    "    idx_n_comp = idx[:n_comp]\n",
    "    \n",
    "    # eigenvector according to top n_comp largest\n",
    "    eig_vector = eig_vector[:, idx_n_comp]\n",
    "    \n",
    "    # 6- Dot multiply the centered matrix with the top N eigenvectors of the covariance matrix\n",
    "    pca = np.dot(covariance, eig_vector)\n",
    "    return pca\n",
    "\n",
    "\n",
    "PCA_calculation(X, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.65392786 -0.2775295 ]\n",
      " [-0.84584087  0.31153366]\n",
      " [ 0.55130929  0.09250983]\n",
      " [ 1.94845944 -0.126514  ]]\n",
      "[2.5171201  0.06621324]\n",
      "[0.97436907 0.02563093]\n",
      "[0.97436907 1.        ]\n",
      "Correlation of PCA Component:\n",
      "(3.0531133177191805e-16, 0.9999999999999997)\n"
     ]
    }
   ],
   "source": [
    "# PCA Computation by Sklearn\n",
    "\n",
    "X = np.array([[1, 1, 1], [1, 2, 1], [1, 3, 2], [1, 4, 3]])\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_r = pca.fit_transform(X)\n",
    "print(X_r)\n",
    "\n",
    "print(pca.explained_variance_)\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.explained_variance_ratio_.cumsum())\n",
    "\n",
    "print('Correlation of PCA Component:')\n",
    "print(scipy.stats.pearsonr(X_r[:, 0], X_r[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity: Apply Principle to Boston Housing Features and then train the linear regression model\n",
    "\n",
    "* Basically, we remove correlation among features with PCA\n",
    "\n",
    "* Steps for doing PCA with Linear Regression for Boston Housing:\n",
    "\n",
    "    1- split dataset: create X_train, y_train, X_test, and y_test\n",
    "    \n",
    "    2- Apply PCA on X_train and obtain the required number of principle components\n",
    "    \n",
    "    3-  `pca = PCA(n_components=2)`\n",
    "    \n",
    "        - `X_train_reduced = pca.fit_transform(X_train)`\n",
    "        \n",
    "    4- Apply data normalization for `X_train_reduced` --> name it as `X_train_reduced_scaled`\n",
    "    \n",
    "    5- Fit the model `lr.fit(X_train_reduced_scaled)`\n",
    "    \n",
    "    6- Pass `X_test_reduced = pca.transform(X_test)` --> then obtain `X_test_reduced_scaled`\n",
    "    \n",
    "    7- Pass `X_test_reduced_scaled` into `lr.predict(X_test_reduced_scaled)`\n",
    "    \n",
    "https://ostwalprasad.github.io/machine-learning/PCA-using-python.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.80457219 0.96727776 0.98868609]\n"
     ]
    }
   ],
   "source": [
    "# Boston Housing Data\n",
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "# load the data\n",
    "boston = load_boston()\n",
    "\n",
    "bos = pd.DataFrame(boston.data)\n",
    "bos.columns = boston.feature_names\n",
    "bos['PRICE'] = boston.target\n",
    "\n",
    "# print(bos.head())\n",
    "\n",
    "print(pca.explained_variance_ratio_.cumsum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_features = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
    "X = bos[col_features]\n",
    "y = bos.PRICE\n",
    "\n",
    "\n",
    "# 1- split dataset: create X_train, y_train, X_test, and y_test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0) \n",
    "\n",
    "\n",
    "# 2- Apply PCA on X_train and obtain the required number of principle components\n",
    "pca = PCA(n_components = 2)\n",
    "X_r = pca.fit_transform(X_train)\n",
    "\n",
    "X_train_reduced = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "bos_reg = LinearRegression()\n",
    "\n",
    "bos_reg.fit(X_r, y_train)\n",
    "\n",
    "# 4- Apply data normalization for X_train_reduced --> name it as X_train_reduced_scaled\n",
    "X_train_reduced_scaled = (X_train_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
