{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification\n",
    "\n",
    "Text classification is to attach labels to bodies of text, e.g., tax document, medical form, etc. based on the text itself\n",
    "\n",
    "Think of your spam folder in your email. How does your email provider know that a particular message is spam or “ham” (not spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: What is the probability that an email be spam? What is the probability that an email be ham?\n",
    "\n",
    "- $P(spam) = ?$\n",
    "\n",
    "- $P(ham) = ?$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: We know an email is spam, what is the probability that password be a word in it? (What is the frequency of password in a spam email?)\n",
    "\n",
    "- Hint: Create the dictionary of spam where its key would be unique words in spam emails and the value shows the occurance of that word\n",
    "- Create two dictionaries for spam and ham wher keys are unique words and values are the frequency for each word\n",
    "    - example: if the word 'password' shows up 4 times in the text, then in the dictionary, they would be 'password' and the value would be 4.\n",
    "   \n",
    "- create the dictionaries programmatically using for loops\n",
    "- use the below text to make your dictionary:\n",
    "    - spam_text = ['Send us your password', review us', 'Send your password', 'Send us your account']\n",
    "    - ham_text = ['Send us your review', 'review your password']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam = {\n",
    "    \"password\": 2,\n",
    "    \"review\": 1,\n",
    "    \"send\": 3,\n",
    "    \"us\": 3,\n",
    "    \"your\": 3,\n",
    "    \"account\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham = {\n",
    "    \"password\": 1,\n",
    "    \"review\": 2,\n",
    "    \"send\": 1,\n",
    "    \"us\": 1,\n",
    "    \"your\": 1,\n",
    "    \"account\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam Dictionary:\n",
      "{'send': 3, 'us': 3, 'your': 3, 'password': 2, 'review': 1, 'account': 1}\n",
      "\n",
      "\n",
      "Ham Dictionary:\n",
      "{'send': 1, 'us': 1, 'your': 2, 'review': 2, 'password': 1}\n"
     ]
    }
   ],
   "source": [
    "spam_text = ['Send us your password', 'review us', 'Send your password', 'Send us your account']\n",
    "ham_text = ['Send us your review', 'review your password']\n",
    "\n",
    "spam= {}\n",
    "             \n",
    "for i in spam_text:\n",
    "    for j in i.lower().split(' '):\n",
    "        if j not in spam:\n",
    "            spam[j] = 1\n",
    "        else:\n",
    "            spam[j] += 1\n",
    "            \n",
    "print(\"Spam Dictionary:\")\n",
    "print(spam)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "ham= {}\n",
    "             \n",
    "for i in ham_text:\n",
    "    for j in i.lower().split(' '):\n",
    "        if j not in ham:\n",
    "            ham[j] = 1\n",
    "        else:\n",
    "            ham[j] += 1\n",
    "            \n",
    "print(\"Ham Dictionary:\")\n",
    "print(ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: Given our dictionaries from the last activity, if we know an email is spam/ham, what is the probability that the word 'password' is in the email?\n",
    "\n",
    "What is the frequency of 'password' in a spam email?\n",
    "\n",
    "* Answer:\n",
    "\n",
    "$P(password \\mid spam) = 2/(2+1+3+3+3+1) = 2/13$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.384615384615385\n"
     ]
    }
   ],
   "source": [
    "p_password_given_spam = spam['password']/sum(spam.values()) * 100\n",
    "print(p_password_given_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.285714285714285\n"
     ]
    }
   ],
   "source": [
    "p_password_given_ham = ham['password']/sum(ham.values()) * 100\n",
    "print(p_password_given_ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: Assume we have seen password in an email, what is the probability that the email be spam?\n",
    "\n",
    "- $P(spam \\mid password) = ?$\n",
    "\n",
    "- Hint: Use Bayes' rule:\n",
    "\n",
    "    * $P(spam \\mid password) = (P(password \\mid spam) P(spam))/ P(password)$ \n",
    "\n",
    "    * $LOTP(Law of Total Probability): P(password) = P(password \\mid spam) P(spam) + P(password \\mid ham) P(ham)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of Password:\n",
      "15.018315018315016\n",
      "\n",
      "\n",
      "Probability of spam given password:\n",
      "0.6829268292682927\n"
     ]
    }
   ],
   "source": [
    "prob_spam = 4/6\n",
    "prob_ham = 2/6\n",
    "\n",
    "#LOTP\n",
    "p_password = (p_password_given_spam * prob_spam) + (p_password_given_ham * prob_ham)\n",
    "print(\"Probability of Password:\")\n",
    "print(p_password)\n",
    "print('\\n')\n",
    "\n",
    "# Bayes Rule\n",
    "p_spam_given_password = (p_password_given_spam*prob_spam) / p_password\n",
    "print(\"Probability of spam given password:\")\n",
    "print(p_spam_given_password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31707317073170727\n"
     ]
    }
   ],
   "source": [
    "p_ham_given_ = 1.0 - 0.6829268292682927\n",
    "print(p_ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of Password:\n",
      "15.018315018315016\n",
      "\n",
      "\n",
      "Probability of ham given password:\n",
      "0.3170731707317073\n"
     ]
    }
   ],
   "source": [
    "prob_spam = 4/6\n",
    "prob_ham = 2/6\n",
    "\n",
    "#LOTP\n",
    "p_password = (p_password_given_ham * prob_ham) + (p_password_given_spam * prob_spam)\n",
    "print(\"Probability of Password:\")\n",
    "print(p_password)\n",
    "print('\\n')\n",
    "\n",
    "# Bayes Rule\n",
    "p_spam_given_password = (p_password_given_ham * prob_ham) / p_password\n",
    "print(\"Probability of ham given password:\")\n",
    "print(p_spam_given_password)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes Classifier\n",
    "The Bayes Theorem : \n",
    "$ P(spam | w_1, w_2, ..., w_n) = {P(w_1, w_2, ..., w_n | spam)}/{P(w_1, w_2, ..., w_n)} $$\n",
    "\n",
    "Naive Bayes assumption is that each word is independent of all other words, In reality, this is not true! But lets try it out for real world examples\n",
    "\n",
    "So the above relations become simple with this assumption: \n",
    "\n",
    "$P(spam | w_1, w_2, ..., w_n) = {P(w_1| spam)P(w_2| spam) ... P(w_n| spam)P(spam)}/{P(w_1, w_2, ..., w_n)}$\n",
    "\n",
    "$P(ham | w_1, w_2, ..., w_n) = {P(w_1| ham)P(w_2| ham) ... P(w_n| ham)P(ham)}/{P(w_1, w_2, ..., w_n)}$\n",
    "\n",
    "The denominator $P(w_1, w_2, ..., w_n)$ is independent of spam and ham, so we can remove it to simplify our equations, as we only care about labeling, and proportional relationships:\n",
    "\n",
    "$ P(spam | w_1, w_2, ..., w_n) \\propto P(spam | w_1, w_2, ..., w_n) = P(w_1| spam)P(w_2| spam) ... P(w_n| spam)P(spam)$\n",
    "\n",
    "$ P(ham | w_1, w_2, ..., w_n) \\propto P(ham | w_1, w_2, ..., w_n) = P(w_1| ham)P(w_2| ham) ... P(w_n| ham)P(ham)$\n",
    "\n",
    "This is easier to expres if we can write is as a summation. To do so, we can take the log of both sides of the equation, because the log of a product is the sum of the logs.\n",
    "\n",
    "$logP(spam | w_1, w_2, ..., w_n) \\propto {\\sum_{i=1}^{n}log P(w_i| spam)+ log P(spam)}$\n",
    "\n",
    "Given the above, we can therefore, say that if:\n",
    "\n",
    "${\\sum_{i=1}^{n}log P(w_i| spam)+ log P(spam)} > {\\sum_{i=1}^{n}log P(w_i| ham)+ log P(ham)}$\n",
    "\n",
    "then the sentence is pam. Otherwise, it is ham!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo-code for Naive Bayes for spam/ham dataset:\n",
    "\n",
    "- Assume the following small dataset is given\n",
    "\n",
    "- The first column is the labels of received emails\n",
    "\n",
    "- The second column is the sentenses, body of the email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-38-47ce483854cb>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-38-47ce483854cb>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    <img src=\"spam_minidataset.png\" width=\"500\" height=\"500\">\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "<img src=\"spam_minidataset.png\" width=\"500\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Based on the given dataset above, create the following two dictionaries:\n",
    "\n",
    "    - Ham -> D_ham = {'Jos': 1,'ask':1, 'you':1,... }\n",
    "    \n",
    "    - Spam- > D_spam= {'Did': 1, 'you':3, ... }\n",
    "    \n",
    "Each dictionary representes all words for the spam and ham emails and their occurance numbers (as the value of dictionaries)\n",
    "\n",
    "2- For any new given sentenses, having $w_1$, $w_2$, ... $w_n$ words, assume the sentences is ham, calculate:\n",
    "\n",
    " $P(w_1| ham)$, $P(w_2| ham)$, ..., $P(w_n| ham)$\n",
    " \n",
    " then $log(P(w_1| ham))$, $log(P(w_2| ham))$, ..., $log(P(w_n| ham))$\n",
    " \n",
    "add them all \n",
    "\n",
    "\n",
    "3- Calculate what percentage of labels is ham -> $P(ham)$ -> take log -> $log(P(ham))$\n",
    "\n",
    "4- Add the value from step (2) and (3)\n",
    "\n",
    "5- Do Steps (2) - (4), and now assume the given new sentences is spam\n",
    "\n",
    "6- Compare the two values, the greater one would be the class\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                               text\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('spam.csv',encoding='latin-1')\n",
    "data = data.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n",
    "data = data.rename(columns={\"v1\":'label', \"v2\":'text'})\n",
    "print(data.head())\n",
    "tags = data[\"label\"]\n",
    "texts = data[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the Naive Bayes core parts in the following Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9641062455132807\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "class SpamDetector(object):\n",
    "    \"\"\"Implementation of Naive Bayes for binary classification\"\"\"\n",
    "\n",
    "    # clean up our string by removing punctuation\n",
    "    def clean(self, s):\n",
    "        translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "        return s.translate(translator)\n",
    "\n",
    "    #  tokenize our string into words\n",
    "    def tokenize(self, text):\n",
    "        text = self.clean(text).lower()\n",
    "        return re.split(\"\\W+\", text)\n",
    "\n",
    "    # count up how many of each word appears in a list of words.\n",
    "    def get_word_counts(self, words):\n",
    "        word_counts = {}\n",
    "        for word in words:\n",
    "            word_counts[word] = word_counts.get(word, 0.0) + 1.0\n",
    "        return word_counts\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"Fit our classifier\n",
    "        Arguments:\n",
    "            X {list} -- list of document contents\n",
    "            y {list} -- correct labels\n",
    "        \"\"\"\n",
    "        self.num_messages = {}\n",
    "        self.log_class_priors = {}\n",
    "        self.word_counts = {}\n",
    "        self.vocab = set()\n",
    "\n",
    "        # Compute log class priors (the probability that any given message is spam/ham),\n",
    "        # by counting how many messages are spam/ham, \n",
    "        # dividing by the total number of messages, and taking the log.\n",
    "        n = len(X)\n",
    "        self.num_messages['spam'] = sum(1 for label in Y if label == 'spam')\n",
    "        self.num_messages['ham'] = sum(1 for label in Y if label == 'ham')\n",
    "        self.log_class_priors['spam'] = math.log(self.num_messages['spam'] / n )\n",
    "        self.log_class_priors['ham'] = math.log(self.num_messages['ham'] / n )\n",
    "        self.word_counts['spam'] = {}\n",
    "        self.word_counts['ham'] = {}\n",
    "\n",
    "        # for each (document, label) pair, tokenize the document into words.\n",
    "        for x, y in zip(X, Y):\n",
    "            c = 'spam' if y == 'spam' else 'ham'\n",
    "            counts = self.get_word_counts(self.tokenize(x))\n",
    "            # For each word, either add it to the vocabulary for spam/ham, \n",
    "            # if it isn’t already there, and update the number of counts. \n",
    "            for word, count in counts.items():\n",
    "                # Add that word to the global vocabulary.\n",
    "                if word not in self.vocab:\n",
    "                    self.vocab.add(word)\n",
    "                if word not in self.word_counts[c]:\n",
    "                    self.word_counts[c][word] = 0.0\n",
    "\n",
    "                self.word_counts[c][word] += count\n",
    "\n",
    "    # function to actually output the class label for new data.\n",
    "    def predict(self, X):\n",
    "        result = []\n",
    "        # Given a document...\n",
    "        for x in X:\n",
    "            counts = self.get_word_counts(self.tokenize(x))\n",
    "            spam_score = 0\n",
    "            ham_score = 0\n",
    "            # We iterate through each of the words...\n",
    "            for word, _ in counts.items():\n",
    "                if word not in self.vocab: continue\n",
    "                # ... and compute log p(w_i|Spam), and sum them all up. The same will happen for Ham\n",
    "                # add Laplace smoothing\n",
    "                # https://medium.com/syncedreview/applying-multinomial-naive-bayes-to-nlp-problems-a-practical-explanation-4f5271768ebf\n",
    "                log_w_given_spam = math.log( (self.word_counts['spam'].get(word, 0.0) + 1) / (self.num_messages['spam'] + len(self.vocab)) )\n",
    "                log_w_given_ham = math.log( (self.word_counts['ham'].get(word, 0.0) + 1) / (self.num_messages['ham'] + len(self.vocab)) )\n",
    "\n",
    "                spam_score += log_w_given_spam\n",
    "                ham_score += log_w_given_ham\n",
    "            \n",
    "            # Then we add the log class priors...\n",
    "            spam_score += self.log_class_priors['spam']\n",
    "            ham_score += self.log_class_priors['ham']\n",
    "\n",
    "            # ... and check to see which score is bigger for that document.\n",
    "            # Whichever is larger, that is the predicted label!\n",
    "            if spam_score > ham_score:\n",
    "                result.append('spam')\n",
    "            else:\n",
    "                result.append('ham')\n",
    "        return result\n",
    "        \n",
    "\n",
    "# TODO: Fill in the below function to make a prediction, \n",
    "# your answer should match the final number in the below output (0.9641)\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    sd = SpamDetector()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "    sd.fit(X_train, y_train)\n",
    "    predictions = np.array(sd.predict(X_test))\n",
    "    # accuracy\n",
    "    print(np.mean(predictions == y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                               text\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
      "5572\n",
      "{'spam': 567, 'ham': 3612}\n",
      "0.9641\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # import/clean/label your data\n",
    "    data = pd.read_csv('spam.csv',encoding='latin-1')\n",
    "    data = data.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n",
    "    data = data.rename(columns={\"v1\":'label', \"v2\":'text'})\n",
    "    print(data.head())\n",
    "    tags = data[\"label\"]\n",
    "    texts = data[\"text\"]\n",
    "\n",
    "    # create texts and tags\n",
    "    X, y = texts, tags\n",
    "    print(len(X))\n",
    "    \n",
    "    # transform text into numerical vectors\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "    \n",
    "    # instantiate your SpamDetector\n",
    "    MNB = SpamDetector()\n",
    "    # fit to model, with the trained part of the dataset\n",
    "    MNB.fit(X_train.values, y_train.values)\n",
    "    print(MNB.num_messages)\n",
    "#     print(MNB.word_counts)\n",
    "    # make predictions\n",
    "    pred = MNB.predict(X_test.values)\n",
    "    true = y_test.values\n",
    "    # test for accuracy\n",
    "    accuracy = sum(1 for i in range(len(pred)) if pred[i] == true[i]) / float(len(pred))\n",
    "    print(\"{0:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: use sklearn CountVectorizer and MultinomialNB to spam email dataset\n",
    "\n",
    "As we've seen with previous topics, sklearn has a lot of built in functionality that can save us from writing the code from scratch. We are going to solve the same problem in the previous activity, but using sklearn!\n",
    "\n",
    "For example, the `SpamDectector` class in the previous activity is an example of a **Multinomial Naive Bayes (MNB) model**. An MNB lets us know that each conditional probability we're looking at (i.e. $P(spam | w_1, w_2, ..., w_n)$) is a multinomial (several terms, polynomial) distribution, rather than another type distribution.\n",
    "\n",
    "**In groups of 3, complete the activity by using the provided starter code and following the steps below:**\n",
    "\n",
    "1 - Split the dataset\n",
    "\n",
    "`from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)`\n",
    "\n",
    "2 - Vectorize the dataset : `vect = CountVectorizer()`\n",
    "\n",
    "3 - Transform training data into a document-term matrix (BoW): `X_train_dtm = vect.fit_transform(X_train)`\n",
    "\n",
    "4 - Build and evaluate the model\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "- Remember how you prepared/cleaned/labeled the dataset, created texts and tags, and split the data innto train vs test from the previous activity. You'll need to do so again here\n",
    "- Review the [CountVectorizer documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) to see how you can transform text into numerical vectors\n",
    "- Need more help? Check out this [MNB Vectorization](https://www.ritchieng.com/machine-learning-multinomial-naive-bayes-vectorization/) article and see what you can use from it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                               text\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
      "  (0, 3286)\t1\n",
      "  (0, 4747)\t2\n",
      "  (0, 1896)\t1\n",
      "  (0, 875)\t2\n",
      "  (0, 6599)\t2\n",
      "  (0, 801)\t1\n",
      "  (0, 5258)\t1\n",
      "  (0, 7209)\t3\n",
      "  (0, 1559)\t1\n",
      "  (0, 913)\t1\n",
      "  (0, 6623)\t3\n",
      "  (0, 1050)\t1\n",
      "  (0, 5980)\t1\n",
      "  (0, 3530)\t1\n",
      "  (0, 919)\t1\n",
      "  (0, 802)\t1\n",
      "  (0, 819)\t1\n",
      "  (0, 5712)\t1\n",
      "  (0, 6727)\t1\n",
      "  (0, 2112)\t1\n",
      "  (0, 5065)\t2\n",
      "  (0, 7373)\t1\n",
      "  (0, 4176)\t2\n",
      "  (0, 1535)\t2\n",
      "  (0, 6604)\t1\n",
      "  :\t:\n",
      "  (4176, 4747)\t1\n",
      "  (4176, 3252)\t1\n",
      "  (4176, 3416)\t1\n",
      "  (4176, 2304)\t1\n",
      "  (4176, 6638)\t1\n",
      "  (4176, 4450)\t1\n",
      "  (4176, 7163)\t1\n",
      "  (4176, 4219)\t1\n",
      "  (4176, 1590)\t1\n",
      "  (4176, 3439)\t1\n",
      "  (4176, 4833)\t1\n",
      "  (4176, 4894)\t1\n",
      "  (4177, 3647)\t1\n",
      "  (4177, 3252)\t1\n",
      "  (4177, 6074)\t1\n",
      "  (4177, 4125)\t1\n",
      "  (4177, 3162)\t1\n",
      "  (4177, 4766)\t1\n",
      "  (4177, 1848)\t1\n",
      "  (4177, 5232)\t1\n",
      "  (4177, 6953)\t1\n",
      "  (4178, 4274)\t1\n",
      "  (4178, 7295)\t1\n",
      "  (4178, 6055)\t1\n",
      "  (4178, 1695)\t1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9856424982053122"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "# prepare the dataset\n",
    "data = pd.read_csv('spam.csv',encoding='latin-1')\n",
    "data = data.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n",
    "data = data.rename(columns={\"v1\":'label', \"v2\":'text'})\n",
    "print(data.head())\n",
    "tags = data[\"label\"]\n",
    "texts = data[\"text\"]\n",
    "\n",
    "# create texts and tags\n",
    "X, y = texts, tags\n",
    "\n",
    "# split the data into train vs test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "# transform text into numerical vectors\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_dtm = vectorizer.fit_transform(X_train)\n",
    "print(X_train_dtm)\n",
    "\n",
    "# instantiate Multinomial Naive Bayes Model\n",
    "nb = MultinomialNB()\n",
    "# fit the model, with the trained part of the dataset\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "X_test_dtm = vectorizer.transform(X_test)\n",
    "# make prediction\n",
    "y_pred_class = nb.predict(X_test_dtm)\n",
    "# test accuracy of prediction\n",
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
